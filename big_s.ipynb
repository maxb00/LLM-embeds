{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding defintions with **S**\n",
    "If we want to define term X, how can we do so based on information extracted from a word embedding? I am proposing and will explore the use of a simple relation of the two common vector descriptors - angle and magnitude. When defining a word, I will assign each word in our vocabulary some kind of score, **S**, that will help us define X with some pruning.\n",
    "```\n",
    "S(X, y) = cosine_similarity(X, y) * |y|\n",
    "```\n",
    "This is based on the idea that words with a larger magnitude appear more frequently in the test set, and therefore are likely to be more simple (or at least common). By using each to scale the other, we project both important pieces of information into the same space, and can begin to construct a definition.\n",
    "\n",
    "How do we avoid orthographic influences? After generating S(X, Y), I will prune out any remaining 'descriptors' that share the same 3-letter sequence to start the word. Hopefully this will reduce \"man\" being defined using \"manner\" or \"manuscript\", or something similar. Once the list is pruned, I will chose the two highest scoring terms in S(X, Y) that do not contain orthographic similarity, and output them as our \"definition\". This will complete the statement \"X is a(n) y1 y2\" (ex. \"bachelor is an unmarried man\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "import sentencepiece    # necessary for proper t5 init.\n",
    "from transformers import T5Tokenizer, T5EncoderModel, GPT2Tokenizer, OPTModel\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# api key set in conda env.\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models\n",
    "You know the drill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "with open('expanded_vocab.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        vocab.append(line.strip())\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPT-1.3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('facebook/opt-1.3b', cache_dir='/scratch/mbarlow6/cache')\n",
    "model_opt_raw = OPTModel.from_pretrained('facebook/opt-1.3b', cache_dir='/scratch/mbarlow6/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_embeds = []\n",
    "with open(u'./opt/1_3B.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        opt_embeds.append([float(x) for x in line.strip().split()])\n",
    "model_opt = dict(zip(vocab, opt_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_embed(text, tokenizer=gpt2_tokenizer, model=model_opt_raw, debug=False):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    if debug:\n",
    "        print('Tokens Requested:')\n",
    "        print(tokenizer.batch_decode(inputs.input_ids[0]))\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = torch.squeeze(outputs.last_hidden_state, dim=0)\n",
    "    return np.array(torch.mean(embeddings[1:], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large', cache_dir='/scratch/mbarlow6/cache')\n",
    "model_t5_raw = T5EncoderModel.from_pretrained('t5-large', cache_dir='/scratch/mbarlow6/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_embeds = []\n",
    "with open('./t5/t5large.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t5_embeds.append([float(x) for x in line.strip().split()])\n",
    "model_t5 = dict(zip(vocab, t5_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_embed(text, tokenizer=t5_tokenizer, model=model_t5_raw, debug=False):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    if debug:\n",
    "        print('Tokens Requested:')\n",
    "        print(tokenizer.batch_decode(inputs.input_ids[0]))\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = torch.squeeze(outputs.last_hidden_state, dim=0)\n",
    "    return np.array(torch.mean(embeddings, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers\n",
    "Positive for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive(words, model='opt'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        words: iterable\n",
    "        model: 'opt', or 't5'\n",
    "    Returns:\n",
    "        Positive (summed vectors) of word embeddings of a given list of words from the specified model. Defaults to GPT-3.\n",
    "    \"\"\"\n",
    "    if isinstance(words, str):\n",
    "        print(f\"You requested the positive of the string \\\"{words}\\\". Did you mean [\\\"{words}\\\"]?\")\n",
    "\n",
    "    out = 0\n",
    "    for token in words:\n",
    "        # convert token to string\n",
    "        word = str(token)\n",
    "        # do model check - least intensive operation to repeat\n",
    "        if model.lower() == 'opt':\n",
    "            if word in model_opt:\n",
    "                ex = model_opt[word]\n",
    "            else:\n",
    "                # squeeze!\n",
    "                ex = opt_embed(word)\n",
    "                model_opt[word] = ex\n",
    "        elif model.lower() == 't5':\n",
    "            if word in model_t5:\n",
    "                ex = model_t5[word]\n",
    "            else:\n",
    "                ex = t5_embed(word)\n",
    "                model_t5[word] = ex\n",
    "        else:\n",
    "            raise ValueError('Please provide either opt or t5 as a model choice.')\n",
    "\n",
    "        # construct positive\n",
    "        if isinstance(out, int):\n",
    "            out = np.array(ex).reshape(1, -1)\n",
    "        else:\n",
    "            out += np.array(ex).reshape(1, -1)\n",
    "            \n",
    "    return out if not isinstance(out, int) else np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigs_define(phrase, n=2, model='opt'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        phrase: iterable or string  -> Word(s) to be defined using big S.\n",
    "        n: int                      -> Number of terms in definition. Defaults to 2.\n",
    "        model: 'opt' or 't5'        -> Model to use. Defaults to OPT.\n",
    "    Returns:\n",
    "        Defintion of phrase using n terms derived from chosen model's weights.\n",
    "    \"\"\"\n",
    "    # normalize phrase to list\n",
    "    if isinstance(phrase, str):\n",
    "        phrase = [phrase]\n",
    "    phrase = positive(phrase, model=model)\n",
    "\n",
    "    # initialize S\n",
    "    S = []\n",
    "\n",
    "    # score vocab\n",
    "    for term in vocab:\n",
    "        b = positive([term], model=model)\n",
    "        \n",
    "        mag = np.sqrt(b.dot(b))\n",
    "        s = cosine_similarity(phrase, b) * mag\n",
    "\n",
    "        S.append((term, s))\n",
    "\n",
    "    # sort S\n",
    "    S.sort(key=lambda x: x[1])\n",
    "\n",
    "    # get answer\n",
    "    ans = []\n",
    "    while len(ans) < n and len(S) > 0:\n",
    "        # check S[0][0] for orthographic similarity\n",
    "        flag = False\n",
    "        for word in phrase:\n",
    "            if word[:3] == S[-1][0][:3]:\n",
    "                flag = True\n",
    "                break\n",
    "        \n",
    "        if flag:\n",
    "            S.pop(-1)\n",
    "            continue\n",
    "\n",
    "        ans.append(S.pop(-1))\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "Now we check if this is viable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigs_define(['puppy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigs_define(['bachelor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigs_define(['wife'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigs_define(['husband'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigs_define(['king'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigs_define(['duckling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigs_define(['kitten'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigs_define(['knowledge'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('aca2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50d097db58378ea4fc243263c82ac89485e39b9e188e1c6006c60e88b9913d6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
