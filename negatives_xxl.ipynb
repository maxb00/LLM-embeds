{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Mikolov Negatives'\n",
    "If we take the vector of king, subtract the vector of man, and add the vector of woman, how close are we to the vector of queen?\n",
    "This notebook will implement a function to allow smooth testing of this experiment.\n",
    "\n",
    "Changes in XXL: No OPT. GPT-3 Davinci and google/flan-t5-xxl (11b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "import sentencepiece    # necessary for proper t5 init.\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# api key set in conda env.\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab\n",
    "Now expanded to the Oxford 5000, plus relevant test words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5124"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = []\n",
    "with open('./vocab/expanded_vocab.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        vocab.append(line.strip())\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3\n",
    "Vectors are normalized. Produced by https://arxiv.org/abs/2201.10005. Note: \"Our models achieved new state-of-the-art results in linear-probe classification, text search and code search. We find that our models **underperformed on sentence similarity tasks and observed unexpected training behavior with respect to these tasks**.\" (emphasis mine) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading saved embeddings from GPT-3 Davinci\n",
    "# Davinci embeds: 12,228 dims\n",
    "dav_embeds = []\n",
    "with open(u'./gpt/gpt_davinci.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        dav_embeds.append([float(x) for x in line.strip().split()])\n",
    "\n",
    "model_gpt = dict(zip(vocab, dav_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for getting new embeddings from OpenAI\n",
    "def gpt_embed(text, engine='text-similarity-davinci-001'):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return openai.Embedding.create(input=[text], engine=engine)['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5\n",
    "Here is our encoder-decoder model. I will extract embeddings from the encoder only, as per results from https://arxiv.org/pdf/2108.08877.pdf. Note: \"When mean pooling is applied to the T5’s encoder outputs, it greatly outperforms the average embeddings of BERT. Notably, even without fine-tuning, the average embeddings of the T5’s encoder-only outputs outperforms SimCSE-RoBERTa, which is fine-tuned on NLI datase.\"\n",
    "\n",
    "Seeing as I amd doing no fine tuning, and mean pool OPT decoder outputs, I will do the same with T5's **encoder** outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/flan-t5-xxl were not used when initializing T5EncoderModel: ['decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'lm_head.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.embed_tokens.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.final_layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-xxl', cache_dir='/scratch/mbarlow6/.cache')\n",
    "model_t5_raw = T5EncoderModel.from_pretrained('google/flan-t5-xxl', cache_dir='/scratch/mbarlow6/.cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_embeds = []\n",
    "with open('./t5/flan_t5_11b.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t5_embeds.append([float(x) for x in line.strip().split()])\n",
    "model_t5 = dict(zip(vocab, t5_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_embed(text, tokenizer=t5_tokenizer, model=model_t5_raw, debug=False):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    if debug:\n",
    "        print('Tokens Requested:')\n",
    "        print(tokenizer.batch_decode(inputs.input_ids[0]))\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = torch.squeeze(outputs.last_hidden_state, dim=0)\n",
    "    return np.array(torch.mean(embeddings, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers\n",
    "Here I will define some functions for the purpose of the investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive(words, model='gpt'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        words: iterable\n",
    "        model: 'gpt' or 't5'\n",
    "    Returns:\n",
    "        Positive (summed vectors) of word embeddings of a given list of words from the specified model. Defaults to GPT-3.\n",
    "    \"\"\"\n",
    "    if isinstance(words, str):\n",
    "        print(f\"You requested the positive of the string \\\"{words}\\\". Did you mean [\\\"{words}\\\"]?\")\n",
    "\n",
    "    out = 0\n",
    "    for token in words:\n",
    "        # convert token to string\n",
    "        word = str(token)\n",
    "        # do model check - least intensive operation to repeat\n",
    "        if model.lower() == 'gpt':\n",
    "            # look for token in cached GPT embeds\n",
    "            if word in model_gpt:\n",
    "                ex = model_gpt[word]  # ex for \"extracted\"\n",
    "            # if not found, query API\n",
    "            else:\n",
    "                ex = gpt_embed(word)\n",
    "                model_gpt[word] = ex\n",
    "        elif model.lower() == 't5':\n",
    "            if word in model_t5:\n",
    "                ex = model_t5[word]\n",
    "            else:\n",
    "                ex = t5_embed(word)\n",
    "                model_t5[word] = ex\n",
    "        else:\n",
    "            raise ValueError('Please provide either gpt, opt, or t5 as a model choice.')\n",
    "\n",
    "        # construct positive\n",
    "        if isinstance(out, int):\n",
    "            out = np.array(ex).reshape(1, -1)\n",
    "        else:\n",
    "            out += np.array(ex).reshape(1, -1)\n",
    "            \n",
    "    return out if not isinstance(out, int) else np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(words, target, vec=False, model='gpt'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        words: iterable or vector   -> Items to be combined into a positive and compared,\n",
    "                                       or already constructed vector of matching dimensions.\n",
    "        target: str     -> single term to calulate similarity to.\n",
    "        vec: bool       -> true if 'words' is a vector\n",
    "        model: str      -> 'gpt' or 't5'\n",
    "    Returns:\n",
    "        The cosine similarity of the words vector and target term in specified model.\n",
    "    \"\"\"\n",
    "    # get phrase\n",
    "    phrase = words\n",
    "    if not vec:\n",
    "        if isinstance(words, str):\n",
    "            phrase = positive([words], model)\n",
    "        else:\n",
    "            phrase = positive(words, model)\n",
    "    \n",
    "    # get target\n",
    "    target = positive([target], model)\n",
    "\n",
    "    return cosine_similarity(phrase, target)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative(start, det, add, end, model='gpt'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        start: str  -> starting word\n",
    "        det: str    -> word to subtract from start\n",
    "        add: str    -> word to add to get new direction\n",
    "        end: str    -> target word\n",
    "        model: str  -> gpt or t5.\n",
    "    Returns:\n",
    "        The cosine similarity between <end> and <start> - <det> + <add>.\n",
    "    \"\"\"\n",
    "    A = positive([start], model)\n",
    "    B = positive([det], model)\n",
    "    C = positive([add], model)\n",
    "    D = positive([end], model)\n",
    "    neg = (A - B) + C\n",
    "    return cosine_similarity(neg, D)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mikolov(start, less, more, target):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        start: str  -> starting word\n",
    "        det: str    -> word to subtract from start\n",
    "        add: str    -> word to add to get new direction\n",
    "        end: str    -> target word\n",
    "    Returns:\n",
    "        None. Wraps negative for each model we have to test.\n",
    "    \"\"\"\n",
    "    print(*[start, less, more], sep=', ', end='')\n",
    "    print(f\" -> {target}\")\n",
    "    for model in ['gpt', 't5']:\n",
    "        print(f\"model: {model}\")\n",
    "        print(f\"score: {negative(start, less, more, model)}\")\n",
    "        print(f\"benchmark: {start} -> {target} = {calculate_similarity([start], target, model=model)}\")\n",
    "        print('-------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "Main Categories identified: *opposite-gender*, *capitol-of*, *pluralization*, *adjective-scale*, *possesion*, and *tense*.\n",
    "\n",
    "Please refer to https://arxiv.org/pdf/1509.01692.pdf and https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* opposite-gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king, man, woman -> queen\n",
      "model: gpt\n",
      "score: 0.7703084983562917\n",
      "benchmark: king -> queen = 0.8985080894387735\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.7258212692616726\n",
      "benchmark: king -> queen = 0.6996615597317424\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('king', 'man', 'woman', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chairman, man, woman -> chairwoman\n",
      "model: gpt\n",
      "score: 0.6888063543967273\n",
      "benchmark: chairman -> chairwoman = 0.9315533026820068\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.6575280485534069\n",
      "benchmark: chairman -> chairwoman = 0.6923004904833964\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('chairman', 'man', 'woman', 'chairwoman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brother, male, female -> sister\n",
      "model: gpt\n",
      "score: 0.7901466172516423\n",
      "benchmark: brother -> sister = 0.9186488343882371\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.7433035811329919\n",
      "benchmark: brother -> sister = 0.8771536292487245\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('brother', 'male', 'female', 'sister')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stallion, male, female -> mare\n",
      "model: gpt\n",
      "score: 0.7406233952109762\n",
      "benchmark: stallion -> mare = 0.8411060014518819\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.6982178873116891\n",
      "benchmark: stallion -> mare = 0.37250359848041403\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('stallion', 'male', 'female', 'mare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* capitol-of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madrid, Spain, France -> Paris\n",
      "model: gpt\n",
      "score: 0.7580520368039784\n",
      "benchmark: Madrid -> Paris = 0.8597558370002809\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.7321430761748646\n",
      "benchmark: Madrid -> Paris = 0.68666672706604\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('Madrid', 'Spain', 'France', 'Paris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pluralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars, car, apple -> apples\n",
      "model: gpt\n",
      "score: 0.7595482455293469\n",
      "benchmark: cars -> apples = 0.814181320219243\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.7286188171448085\n",
      "benchmark: cars -> apples = 0.5622069835662842\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('cars', 'car', 'apple', 'apples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "years, year, law -> laws\n",
      "model: gpt\n",
      "score: 0.8080466383177447\n",
      "benchmark: years -> laws = 0.8577423710934432\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.7627040460783938\n",
      "benchmark: years -> laws = 0.542034387588501\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('years', 'year', 'law', 'laws')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjective-scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hot, warm, cool -> cold\n",
      "model: gpt\n",
      "score: 0.7600496905453323\n",
      "benchmark: hot -> cold = 0.8196469902160869\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.7095308319680024\n",
      "benchmark: hot -> cold = 0.7367349160417564\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('hot', 'warm', 'cool', 'cold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best, good, bad -> worst\n",
      "model: gpt\n",
      "score: 0.7348750679534736\n",
      "benchmark: best -> worst = 0.8107603269753152\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.7027772168983535\n",
      "benchmark: best -> worst = 0.762467757632748\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('best', 'good', 'bad', 'worst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* possession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city's, city, bank -> bank's\n",
      "model: gpt\n",
      "score: 0.818953208882454\n",
      "benchmark: city's -> bank's = 0.8540676399020821\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.772950553007265\n",
      "benchmark: city's -> bank's = 0.7040677070617676\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('city\\'s', 'city', 'bank', 'bank\\'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mine, me, you -> yours\n",
      "model: gpt\n",
      "score: 0.8208659247052847\n",
      "benchmark: mine -> yours = 0.9076210077153537\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.7673237665263305\n",
      "benchmark: mine -> yours = 0.6406882706185066\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('mine', 'me', 'you', 'yours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* verb-source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking, legs, teeth -> chewing\n",
      "model: gpt\n",
      "score: 0.7054636865837388\n",
      "benchmark: walking -> chewing = 0.8131267858240695\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.6845487541675309\n",
      "benchmark: walking -> chewing = 0.45184335868969294\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('walking', 'legs', 'teeth', 'chewing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk, legs, teeth -> chew\n",
      "model: gpt\n",
      "score: 0.7205202625033159\n",
      "benchmark: walk -> chew = 0.8141899916189553\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.6997145450187889\n",
      "benchmark: walk -> chew = 0.6378753246466464\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('walk', 'legs', 'teeth', 'chew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sailing, boat, car -> driving\n",
      "model: gpt\n",
      "score: 0.7904261042444086\n",
      "benchmark: sailing -> driving = 0.8523761563932656\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.7329003237907269\n",
      "benchmark: sailing -> driving = 0.5727941778241692\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('sailing', 'boat', 'car', 'driving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sail, boat, car -> drive\n",
      "model: gpt\n",
      "score: 0.7942699441735251\n",
      "benchmark: sail -> drive = 0.8471531312715113\n",
      "-------------------------------------------------------\n",
      "model: t5\n",
      "score: 0.7359975380845648\n",
      "benchmark: sail -> drive = 0.5849099740684152\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mikolov('sail', 'boat', 'car', 'drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Observations\n",
    "\n",
    "This file uses flan-t5-xxl as it's embedding source. \n",
    "\n",
    "While the new vector may score higher in similarity on a case like 'years' - 'year' + 'law' = 'laws' than the benchmark of 'years' -> 'laws', each composite vector is scoring about 75% (give or take) similarity. This makes me feel like we are not truly getting closer but moving towards something decisively more average, but not the target. Why do we never see a score about 85, 90%?\n",
    "\n",
    "This paper has been on my mind - https://doi.org/10.18653/v1%2F2021.emnlp-main.372. Per the abstract, \n",
    ">We find that a small number of rogue dimensions, often just 1-3, dominate these measures. Moreover, we find a striking mismatch between the dimensions that dominate similarity measures and those which are important to the behavior of the model. We show that simple postprocessing techniques such as standardization are able to correct for rogue dimensions and reveal underlying representational quality. We argue that accounting for rogue dimensions is essential for any similarity-based analysis of contextual language models.\n",
    "\n",
    "which leads me to believe embeddings from t5 and opt will not be performant on this task (no matter the size) without post-processing.\n",
    "\n",
    "\n",
    "I believe GPT-3 embeddings undergo some kind of post-processing like this paper suggests, which could explain our significant results with Curie but not with OPT or t5 equivalents."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da6a7ae2d923bd58f8700e06e618ae11c12ea5afaa281ac0423001bc638b87ec"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('vocab': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50d097db58378ea4fc243263c82ac89485e39b9e188e1c6006c60e88b9913d6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
